if (!require("readxl")) install.packages("readxl")
if (!require("tm")) install.packages("tm")
if (!require("topicmodels")) install.packages("topicmodels")
if (!require("tidyverse")) install.packages("tidyverse")
if (!require("tidytext")) install.packages("tidytext")
if (!require("reshape2")) install.packages("reshape2")
if (!require("wordcloud")) install.packages("wordcloud")
if (!require("RColorBrewer")) install.packages("RColorBrewer")
if (!require("rvest")) install.packages("rvest")
if (!require("SnowballC")) install.packages("SnowballC")
if (!require("hunspell")) install.packages("hunspell")
if (!require("textmineR")) install.packages("textmineR")
if (!require("LDAvis")) install.packages("LDAvis")
library(readxl)
library(tm)
library(topicmodels)
library(tidyverse)
library(tidytext)
library(reshape2)
library(wordcloud)
library(RColorBrewer)
library(rvest)
library(SnowballC)
library(hunspell)
library(textmineR)
library(LDAvis)
url <- "https://en.prothomalo.com/"
page <- read_html(url)
titles <- page %>%
html_nodes("h2 a, h3 a") %>%
html_text(trim = TRUE)
print(head(titles, 10))
links <- page %>%
html_nodes("h2 a, h3 a") %>%
html_attr("href")
links <- ifelse(startsWith(links, "http"), links, paste0("https://en.prothomalo.com", links))
print(head(links, 10))
get_article_text <- function(link) {
tryCatch({
article <- read_html(link)
paragraphs <- article %>%
html_nodes("article p, div.story-content p") %>%
html_text(trim = TRUE)
full_text <- paste(paragraphs, collapse = " ")
if (nchar(full_text) == 0) return(NA) else return(full_text)
}, error = function(e) {
return(NA)
})
}
article_texts <- sapply(links[1:10], get_article_text)
print(article_texts)
prothom_data <- tibble(
title = titles[1:10],
link = links[1:10],
content = article_texts
)
print(prothom_data)
write.csv(prothom_data, "prothomalo_top5_articles.csv", row.names = FALSE)
raw_corpus <- prothom_data$content
print(head(raw_corpus, 2))
write.csv(data.frame(raw_corpus = raw_corpus),
"prothomalo_raw_corpus.csv",
row.names = FALSE)
clean_text <- function(text) {
text <- gsub("https?://\\S+|www\\.\\S+", "", text)
text <- tolower(text)
text <- gsub("[[:punct:]]+", " ", text)
text <- gsub("[0-9]+", "", text)
text <- gsub("[[:cntrl:]]", " ", text)
text <- gsub("\\s+", " ", text)
text <- trimws(text)
return(text)
}
cleaned_texts <- sapply(raw_corpus, clean_text)
print(head(cleaned_texts, 1))
tokenize_text <- function(text) {
tokens <- unlist(strsplit(text, "\\s+"))
return(tokens)
}
tokens_list <- lapply(cleaned_texts, tokenize_text)
print(head(tokens_list[[1]], 10))
remove_stopwords <- function(tokens) {
tokens <- tokens[!tolower(tokens) %in% stopwords("en")]
return(tokens)
}
filtered_tokens_list <- lapply(tokens_list, remove_stopwords)
print(head(filtered_tokens_list[[1]], 10))
stem_tokens <- function(tokens) {
tokens <- wordStem(tokens)
return(tokens)
}
stemmed_tokens_list <- lapply(filtered_tokens_list, stem_tokens)
print(head(stemmed_tokens_list[[1]], 10))
remove_emojis <- function(tokens) {
tokens <- gsub("[^\x01-\x7f]", "", tokens)
return(tokens)
}
emoji_cleaned_tokens <- lapply(stemmed_tokens_list, function(tokens) sapply(tokens, remove_emojis))
contains_non_ascii <- function(tokens) {
any(grepl("[^\x01-\x7F]", tokens))
}
any(sapply(emoji_cleaned_tokens, contains_non_ascii))
updated_corpus <- sapply(emoji_cleaned_tokens, function(tokens) paste(tokens, collapse = " "))
names(updated_corpus) <- NULL
print(head(updated_corpus, 1))
write.csv(data.frame(updated_corpus = updated_corpus),
"prothomalo_updated_corpus_new.csv",
row.names = FALSE)
corpus_data <- read.csv("prothomalo_updated_corpus.csv", stringsAsFactors = FALSE)
news <- corpus_data$updated_corpus
corpus_data <- read.csv("prothomalo_updated_corpus.csv", stringsAsFactors = FALSE)
news <- corpus_data$updated_corpus
top_word_count <- 8
topic_number <- 6
corpus <- Corpus(VectorSource(news))
dtm <- DocumentTermMatrix(corpus)
dtm <- removeSparseTerms(dtm, 0.95)
dtm_matrix <- as.matrix(dtm)
non_empty_docs <- rowSums(as.matrix(dtm)) > 0
dtm <- dtm[non_empty_docs, ]
lda_model <- LDA(dtm, k = topic_number, control = list(seed = 123))
topics <- tidy(lda_model, matrix = "beta") %>%
group_by(topic) %>%
slice_max(beta, n = top_word_count, with_ties = FALSE) %>%
ungroup()
topic_proportions <- tidy(lda_model, matrix = "gamma") %>%
mutate(document = as.numeric(document))
p_beta <- topics %>%
ggplot(aes(x = reorder(term, beta), y = beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
labs(
title = "Top Terms for Each Topic",
x = "Terms",
y = "Beta"
)
p_gamma <- topic_proportions %>%
ggplot(aes(x = factor(topic), y = gamma, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ document, scales = "free") +
labs(
title = "Topic Proportions for Each Document",
x = "Topics",
y = "Proportion (Gamma)"
)
print(p_beta)
print(p_gamma)
ggsave("topic_proportions_plot.png", plot = p_gamma, width = 12, height = 8, dpi = 400)
ggsave("top_terms_per_topic.png", plot = p_beta, width = 12, height = 8, dpi = 400)
getwd()
unique_topics <- unique(topics$topic)
for (i in unique_topics) {
topic_terms <- topics %>%
filter(topic == i)
wordcloud(
words = topic_terms$term,
freq = topic_terms$beta,
min.freq = min(topic_terms$beta),
max.words = top_word_count,
colors = brewer.pal(8, "Dark2"),
random.order = FALSE,
scale = c(3, 0.5)
)
title(main = paste("Word Cloud for Topic", i))
}
getwd()
View(dtm_matrix)
if (!require("readxl")) install.packages("readxl")
if (!require("tm")) install.packages("tm")
if (!require("topicmodels")) install.packages("topicmodels")
if (!require("tidyverse")) install.packages("tidyverse")
if (!require("tidytext")) install.packages("tidytext")
if (!require("reshape2")) install.packages("reshape2")
if (!require("wordcloud")) install.packages("wordcloud")
if (!require("RColorBrewer")) install.packages("RColorBrewer")
if (!require("rvest")) install.packages("rvest")
if (!require("SnowballC")) install.packages("SnowballC")
if (!require("hunspell")) install.packages("hunspell")
if (!require("textmineR")) install.packages("textmineR")
if (!require("LDAvis")) install.packages("LDAvis")
library(readxl)
library(tm)
library(topicmodels)
library(tidyverse)
library(tidytext)
library(reshape2)
library(wordcloud)
library(RColorBrewer)
library(rvest)
library(SnowballC)
library(hunspell)
library(textmineR)
library(LDAvis)
url <- "https://en.prothomalo.com/"
page <- read_html(url)
titles <- page %>%
html_nodes("h2 a, h3 a") %>%
html_text(trim = TRUE)
print(head(titles, 10))
links <- page %>%
html_nodes("h2 a, h3 a") %>%
html_attr("href")
links <- ifelse(startsWith(links, "http"), links, paste0("https://en.prothomalo.com", links))
print(head(links, 10))
get_article_text <- function(link) {
tryCatch({
article <- read_html(link)
paragraphs <- article %>%
html_nodes("article p, div.story-content p") %>%
html_text(trim = TRUE)
full_text <- paste(paragraphs, collapse = " ")
if (nchar(full_text) == 0) return(NA) else return(full_text)
}, error = function(e) {
return(NA)
})
}
article_texts <- sapply(links[1:10], get_article_text)
print(article_texts)
prothom_data <- tibble(
title = titles[1:10],
link = links[1:10],
content = article_texts
)
print(prothom_data)
write.csv(prothom_data, "prothomalo_top5_articles.csv", row.names = FALSE)
raw_corpus <- prothom_data$content
print(head(raw_corpus, 2))
write.csv(data.frame(raw_corpus = raw_corpus),
"prothomalo_raw_corpus.csv",
row.names = FALSE)
clean_text <- function(text) {
text <- gsub("https?://\\S+|www\\.\\S+", "", text)
text <- tolower(text)
text <- gsub("[[:punct:]]+", " ", text)
text <- gsub("[0-9]+", "", text)
text <- gsub("[[:cntrl:]]", " ", text)
text <- gsub("\\s+", " ", text)
text <- trimws(text)
return(text)
}
cleaned_texts <- sapply(raw_corpus, clean_text)
print(head(cleaned_texts, 1))
tokenize_text <- function(text) {
tokens <- unlist(strsplit(text, "\\s+"))
return(tokens)
}
tokens_list <- lapply(cleaned_texts, tokenize_text)
print(head(tokens_list[[1]], 10))
remove_stopwords <- function(tokens) {
tokens <- tokens[!tolower(tokens) %in% stopwords("en")]
return(tokens)
}
filtered_tokens_list <- lapply(tokens_list, remove_stopwords)
print(head(filtered_tokens_list[[1]], 10))
stem_tokens <- function(tokens) {
tokens <- wordStem(tokens)
return(tokens)
}
stemmed_tokens_list <- lapply(filtered_tokens_list, stem_tokens)
print(head(stemmed_tokens_list[[1]], 10))
remove_emojis <- function(tokens) {
tokens <- gsub("[^\x01-\x7f]", "", tokens)
return(tokens)
}
emoji_cleaned_tokens <- lapply(stemmed_tokens_list, function(tokens) sapply(tokens, remove_emojis))
contains_non_ascii <- function(tokens) {
any(grepl("[^\x01-\x7F]", tokens))
}
any(sapply(emoji_cleaned_tokens, contains_non_ascii))
updated_corpus <- sapply(emoji_cleaned_tokens, function(tokens) paste(tokens, collapse = " "))
names(updated_corpus) <- NULL
print(head(updated_corpus, 1))
write.csv(data.frame(updated_corpus = updated_corpus),
"prothomalo_updated_corpus_new.csv",
row.names = FALSE)
corpus_data <- read.csv("prothomalo_updated_corpus.csv", stringsAsFactors = FALSE)
news <- corpus_data$updated_corpus
top_word_count <- 8
topic_number <- 6
corpus <- Corpus(VectorSource(news))
dtm <- DocumentTermMatrix(corpus)
dtm <- removeSparseTerms(dtm, 0.95)
dtm_matrix <- as.matrix(dtm)
non_empty_docs <- rowSums(as.matrix(dtm)) > 0
dtm <- dtm[non_empty_docs, ]
lda_model <- LDA(dtm, k = topic_number, control = list(seed = 123))
topics <- tidy(lda_model, matrix = "beta") %>%
group_by(topic) %>%
slice_max(beta, n = top_word_count, with_ties = FALSE) %>%
ungroup()
topic_proportions <- tidy(lda_model, matrix = "gamma") %>%
mutate(document = as.numeric(document))
p_beta <- topics %>%
ggplot(aes(x = reorder(term, beta), y = beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
labs(
title = "Top Terms for Each Topic",
x = "Terms",
y = "Beta"
)
p_gamma <- topic_proportions %>%
ggplot(aes(x = factor(topic), y = gamma, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ document, scales = "free") +
labs(
title = "Topic Proportions for Each Document",
x = "Topics",
y = "Proportion (Gamma)"
)
print(p_beta)
print(p_gamma)
ggsave("topic_proportions_plot.png", plot = p_gamma, width = 12, height = 8, dpi = 400)
ggsave("top_terms_per_topic.png", plot = p_beta, width = 12, height = 8, dpi = 400)
getwd()
unique_topics <- unique(topics$topic)
for (i in unique_topics) {
topic_terms <- topics %>%
filter(topic == i)
wordcloud(
words = topic_terms$term,
freq = topic_terms$beta,
min.freq = min(topic_terms$beta),
max.words = top_word_count,
colors = brewer.pal(8, "Dark2"),
random.order = FALSE,
scale = c(3, 0.5)
)
title(main = paste("Word Cloud for Topic", i))
}
getwd()
print(p_beta)
View(dtm_matrix)
lda_model <- LDA(dtm, k = topic_number, control = list(seed = 123))
topics <- tidy(lda_model, matrix = "beta") %>%
group_by(topic) %>%
slice_max(beta, n = top_word_count, with_ties = FALSE) %>%
ungroup()
topic_proportions <- tidy(lda_model, matrix = "gamma") %>%
mutate(document = as.numeric(document))
p_beta <- topics %>%
ggplot(aes(x = reorder(term, beta), y = beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
labs(
title = "Top Terms for Each Topic",
x = "Terms",
y = "Beta"
)
p_gamma <- topic_proportions %>%
ggplot(aes(x = factor(topic), y = gamma, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ document, scales = "free") +
labs(
title = "Topic Proportions for Each Document",
x = "Topics",
y = "Proportion (Gamma)"
)
if (!require("readxl")) install.packages("readxl")
if (!require("tm")) install.packages("tm")
if (!require("topicmodels")) install.packages("topicmodels")
if (!require("tidyverse")) install.packages("tidyverse")
if (!require("tidytext")) install.packages("tidytext")
if (!require("reshape2")) install.packages("reshape2")
if (!require("wordcloud")) install.packages("wordcloud")
if (!require("RColorBrewer")) install.packages("RColorBrewer")
if (!require("rvest")) install.packages("rvest")
if (!require("SnowballC")) install.packages("SnowballC")
if (!require("hunspell")) install.packages("hunspell")
if (!require("textmineR")) install.packages("textmineR")
if (!require("LDAvis")) install.packages("LDAvis")
library(readxl)
library(tm)
library(topicmodels)
library(tidyverse)
library(tidytext)
library(reshape2)
library(wordcloud)
library(RColorBrewer)
library(rvest)
library(SnowballC)
library(hunspell)
library(textmineR)
library(LDAvis)
corpus_data <- read.csv("prothomalo_updated_corpus.csv", stringsAsFactors = FALSE)
news <- corpus_data$updated_corpus
top_word_count <- 8
topic_number <- 6
corpus <- Corpus(VectorSource(news))
dtm <- DocumentTermMatrix(corpus)
dtm <- removeSparseTerms(dtm, 0.95)
dtm_matrix <- as.matrix(dtm)
non_empty_docs <- rowSums(as.matrix(dtm)) > 0
dtm <- dtm[non_empty_docs, ]
lda_model <- LDA(dtm, k = topic_number, control = list(seed = 123))
topics <- tidy(lda_model, matrix = "beta") %>%
group_by(topic) %>%
slice_max(beta, n = top_word_count, with_ties = FALSE) %>%
ungroup()
topic_proportions <- tidy(lda_model, matrix = "gamma") %>%
mutate(document = as.numeric(document))
p_beta <- topics %>%
ggplot(aes(x = reorder(term, beta), y = beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
labs(
title = "Top Terms for Each Topic",
x = "Terms",
y = "Beta"
)
p_gamma <- topic_proportions %>%
ggplot(aes(x = factor(topic), y = gamma, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ document, scales = "free") +
labs(
title = "Topic Proportions for Each Document",
x = "Topics",
y = "Proportion (Gamma)"
)
print(p_beta)
print(p_gamma)
ggsave("topic_proportions_plot.png", plot = p_gamma, width = 12, height = 8, dpi = 400)
ggsave("top_terms_per_topic.png", plot = p_beta, width = 12, height = 8, dpi = 400)
getwd()
unique_topics <- unique(topics$topic)
for (i in unique_topics) {
topic_terms <- topics %>%
filter(topic == i)
wordcloud(
words = topic_terms$term,
freq = topic_terms$beta,
min.freq = min(topic_terms$beta),
max.words = top_word_count,
colors = brewer.pal(8, "Dark2"),
random.order = FALSE,
scale = c(3, 0.5)
)
title(main = paste("Word Cloud for Topic", i))
}
getwd()
